from imbami.utils.base_mitigation_method import BaseMitigationMethod
import pandas as pd
import numpy as np

class crbSMOGN(BaseMitigationMethod):
    def __init__(self,
                data: pd.DataFrame,
                target_column: str,
                relevance_values: pd.Series):
        '''
        data : pd.Dataframe
            Dataframe containing the data to be sampled.
        target_column : str
            Name of the target variable.
        relevance_values : pd.Series
            Series containing the respective relevance values to the samples/rows in data.
        '''
        super().__init__(data, target_column, relevance_values)
        
    def run_sampling(self,
                    min_acceptable_relevance: float = 1.0,
                    max_acceptable_relevance: float = 1.0,
                    num_bins: int = 10,
                    allowed_bin_deviation: int = 1,
                    noise_factor: float = 0.01,
                    ignore_categorical_similarity: bool = False,
                    enable_undersampling: bool = True) -> pd.DataFrame:
        """
        Applies crbSMOGN to the given dataset,
        performing both oversampling and undersampling to handle imbalanced regression tasks.

        Parameters:
        ----------        
        min_acceptable_relevance : float, optional
            The minimum acceptable relevance for a sample to be retained without changes. Default is 1.0.
        
        max_acceptable_relevance : float, optional
            The maximum acceptable relevance for a sample to be retained without changes. Default is 1.0.
        
        num_bins : int, optional
            The number of bins to use for discretizing numeric columns. Default is 10.
        
        allowed_bin_deviation : int, optional
            The maximum allowed deviation in binning to consider samples similar. Default is 1.
        
        noise_factor : float, optional
            The multiplier applied to the standard deviations when adding Gaussian noise to numeric columns. Default is 0.01.
        
        ignore_categorical_similarity : bool, optional
            If True, categorical features are ignored when calculating similarity between samples. Default is False.
        
        enable_undersampling : bool, optional
            If True, undersampling is performed on the dataset. Default is True.

        Returns:
        -------
        new_dataset : pd.DataFrame
            The new dataset with the synthetic samples added and undersampling applied (if enabled).
 
        Notes:
        -----
            - num_gaussian_samples: int, the number of samples generated using Gaussian noise.
            - num_knn_samples: int, the number of samples generated through interpolation with nearest neighbors.
            - num_dropped_samples: int, the number of samples dropped due to undersampling.
        """
        self.min_acceptable_relevance = min_acceptable_relevance
        self.max_acceptable_relevance = max_acceptable_relevance
        self.allowed_bin_deviation = allowed_bin_deviation
        self.ignore_categorical_similarity = ignore_categorical_similarity
        self.noise_factor = noise_factor
        self.num_bins = num_bins

        # Discretize the dataset for similarity calculations
        self.binned_data = self.discretize_dataset(
            self.data, self.num_bins, self.numeric_columns, self.categorical_columns, self.ignore_categorical_similarity).to_numpy()

        # Decide which samples to retain, oversample, or undersample
        self.sample_treatment = pd.Series(index=self.data.index, dtype=object)
        self.sample_treatment = self.relevance_values.apply(
            lambda relv: self.check_treatment(relv))

        # Identify samples that require oversampling
        self.oversample_indices = self.data.index.get_indexer(self.sample_treatment[self.sample_treatment > 1].index)
        total_oversample_count = (self.sample_treatment[self.sample_treatment > 1] - 1).sum()
        oversampled_data = np.zeros((total_oversample_count, self.data_numpy.shape[1]))

        # Counters for the number of samples generated by each method
        self.num_oversampled_samples = 0
        self.num_gaussian_samples = 0
        self.num_knn_samples = 0

        # Perform oversampling
        for idx in self.oversample_indices:
            num_replications = int(self.sample_treatment.iloc[idx]) - 1  # Adjust for existing sample

            # Generate synthetic samples using interpolation and Gaussian noise
            new_samples, gaussian_count, knn_count = self.oversample(reference_sample_index= idx,
                                                                num_replications=num_replications)

            # Store the new samples
            oversampled_data[self.num_oversampled_samples:self.num_oversampled_samples + num_replications, :] = new_samples
            self.num_oversampled_samples += num_replications
            self.num_gaussian_samples += gaussian_count
            self.num_knn_samples += knn_count

        # Postprocessing. Back to Pandas.
        oversampled_data = pd.DataFrame(data = oversampled_data, columns= self.data.columns, index= range(self.data.index.max(), self.data.index.max() + oversampled_data.shape[0]))
        oversampled_data = oversampled_data.astype(self.data.dtypes)
        new_dataset = pd.concat([self.data, oversampled_data], axis=0)

        # Perform undersampling if enabled
        if enable_undersampling:
            undersample_indices = self.sample_treatment[self.sample_treatment == 0].index
            new_dataset = new_dataset.drop(undersample_indices, axis=0)
            self.num_dropped_samples = len(undersample_indices)
        else:
            self.num_dropped_samples = 0

        return new_dataset



    def check_treatment(self, relevance: float) -> int:
        """
        Determine the sampling treatment for a sample based on its relevance.

        This function decides the treatment for a sample based on its relevance. The possible outcomes are:
        - `1`: Keep the sample as is.
        - `0`: Drop the sample (undersample).
        - `x`: Replicate the sample `x-1` times (oversample).

        Parameters:
        ----------
        relevance : float
            The ratio based relevance of the sample.

        Returns:
        -------
        int
            A value indicating the treatment of the sample:
            - `1` if the sampling rate is between `min_acceptable_relevance` and `max_acceptable_relevance`.
            - `0` or a positive integer based on the rate and a random value if outside the acceptable range.
        """
        if self.min_acceptable_relevance < relevance < self.max_acceptable_relevance:
            return 1
        else:
            remainder = relevance % 1
            random_value = np.random.uniform()  # Generates a random number between [0, 1]
            if remainder > random_value:
                return int(np.ceil(relevance))
            else:
                return int(np.floor(relevance))
        


    def oversample(self, reference_sample_index: int, 
                    num_replications: int) -> tuple[np.ndarray, int, int]:
        # get similar samples indice (rows) based on the discretized data
        similar_rows = self.get_similar_samples(
                index = reference_sample_index, 
                binned_data = self.binned_data, 
                numerical_mask =self.numerical_mask, 
                categorical_mask =self.categorical_mask, 
                allowed_bin_deviation = self.allowed_bin_deviation, 
                ignore_categoricals = self.ignore_categorical_similarity
            ) 
        # get the samples to the corresponding indices 
        similar_samples = self.data_numpy[similar_rows]

        # check if enough similar samples where found. If yes, sort them by distance. If not, use all for oversampling
        if similar_samples.shape[0] > num_replications:
            num_gaussian_samples = 0
            distances = self.heom_distance(x=self.data_numpy[reference_sample_index],
                                y=similar_samples,
                                feature_ranges= self.feature_ranges,
                                categorical_mask=self.categorical_mask,
                                numerical_mask=self.numerical_mask)
            nearest_neighbors = similar_samples[np.argsort(distances)[:num_replications]]
            num_knn_samples = num_replications
        else:
            nearest_neighbors = similar_samples
            num_knn_samples = nearest_neighbors.shape[0]
            num_gaussian_samples = num_replications - num_knn_samples
            
            

        new_samples = np.zeros(shape=(num_replications, self.data_numpy.shape[1])) # is initialized as object
        replication_counter = 0

        for neighbor_sample in nearest_neighbors:
            new_samples[replication_counter, :] = self.interpolate_sample(x=self.data_numpy[reference_sample_index],
                                                                    y=neighbor_sample,
                                                                    feature_ranges= self.feature_ranges,
                                                                    categorical_mask= self.categorical_mask,
                                                                    numerical_mask=self.numerical_mask)
            replication_counter += 1

        new_samples[replication_counter:, :] = self.add_gaussian_noise(x=self.data_numpy[reference_sample_index],
                                                                standard_deviations= self.standard_deviations,
                                                                noise_factor= self.noise_factor,
                                                                n_samples= num_gaussian_samples,
                                                                numerical_mask= self.numerical_mask,
                                                                categorical_mask=self.categorical_mask
                                                                )

        replication_counter += num_gaussian_samples

        return new_samples, num_gaussian_samples, num_knn_samples